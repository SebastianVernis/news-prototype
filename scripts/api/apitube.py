#!/usr/bin/env python3
"""
Script para descargar noticias de APITube.io
API: https://apitube.io
Registro: https://apitube.io/register
Ventaja: Incluye body completo sin scraping adicional
"""

import os
import sys
import requests
from dotenv import load_dotenv

# Agregar directorio padre al path para imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.utils import normalize_article, save_articles, print_summary

load_dotenv()

API_KEY = os.getenv('APITUBE_KEY')
BASE_URL = 'https://apitube.io/api/v1/news'
HEADERS = {'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'}


def fetch_apitube(country: str = 'mx',
                  category: str = 'politics',
                  language: str = 'es',
                  page_size: int = 20,
                  silent: bool = False) -> list:
    """
    Descarga noticias de APITube.io
    
    Args:
        country: C√≥digo de pa√≠s (mx, us, etc.)
        category: Categor√≠a de noticias (politics, business, etc.)
        language: C√≥digo de idioma
        page_size: N√∫mero de art√≠culos a obtener
        
    Returns:
        Lista de art√≠culos descargados
    """
    if not API_KEY:
        raise ValueError("‚ùå APITUBE_KEY no encontrada en .env")
    
    if not silent:
        print(f"\n{'='*70}")
        print("üì• Descargando noticias de APITube.io")
        print(f"{'='*70}")
        print(f"üåç Pa√≠s: {country.upper()}")
        print(f"üìÇ Categor√≠a: {category}")
        print(f"üåê Idioma: {language}")
        print(f"üìä Cantidad: {page_size}")
    
    params = {
        'country': country,
        'category': category,
        'language': language,
        'pageSize': page_size,
        'apiKey': API_KEY
    }
    
    try:
        response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        articles = data.get('data', [])
        
        if not articles:
            if not silent:
                print("‚ö†Ô∏è  No se encontraron art√≠culos")
            return []
        
        if not silent:
            print(f"‚úÖ Descargados {len(articles)} art√≠culos con body completo")
        
        # Normalizar estructura
        normalized = []
        for article in articles:
            normalized.append(normalize_article(article, 'apitube'))
        
        # Guardar resultados solo si no es modo silencioso
        if not silent:
            json_file, csv_file = save_articles(normalized, 'apitube')
            print_summary(normalized, 'APITube.io', json_file, csv_file)
        
        return normalized
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de conexi√≥n: {e}")
        raise
    except Exception as e:
        print(f"‚ùå Error procesando datos: {e}")
        raise


def main():
    """Funci√≥n principal para ejecutar el script"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Descargar noticias de APITube.io')
    parser.add_argument('--country', type=str, default='mx',
                       help='C√≥digo de pa√≠s (default: mx)')
    parser.add_argument('--category', type=str, default='politics',
                       help='Categor√≠a (default: politics)')
    parser.add_argument('--language', type=str, default='es',
                       help='C√≥digo de idioma (default: es)')
    parser.add_argument('--size', type=int, default=20,
                       help='N√∫mero de art√≠culos (default: 20)')
    
    args = parser.parse_args()
    
    try:
        articles = fetch_apitube(
            country=args.country,
            category=args.category,
            language=args.language,
            page_size=args.size
        )
        
        print(f"üéâ Proceso completado: {len(articles)} art√≠culos guardados")
        
    except Exception as e:
        print(f"\n‚ùå Error fatal: {e}")
        exit(1)


if __name__ == '__main__':
    main()
